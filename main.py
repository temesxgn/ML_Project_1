# -*- coding: utf-8 -*-
"""Project_1_Final_Submission_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L0yMK10Q4XbduS-NAKY4Wjj4A-sGoW8b
"""

import pandas as pd
import numpy as np
from keras.layers import Dense, BatchNormalization, Dropout, Conv2D, Activation, MaxPooling2D, Flatten
from keras.losses import sparse_categorical_crossentropy
from keras.models import Sequential
from keras.optimizers import Adadelta
from keras_preprocessing.image import ImageDataGenerator, img_to_array, load_img
from PIL import Image
import glob
from tqdm import tqdm
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import RepeatedKFold, train_test_split
import matplotlib.pyplot as plt
from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout
from keras.optimizers import Adam, SGD, RMSprop
from keras.models import Model, Input
from keras.applications import xception
from google_drive_downloader import GoogleDriveDownloader as gd
import datetime

num_of_breeds = 50
image_size = 224
batch_size = 16
seed = 2019

gd.download_file_from_google_drive(file_id='1-7TMkVvxx6YaGewdXvbvUHsUP1i-qell', dest_path='./data/images.zip',
                                   unzip=True)
df = pd.read_csv('https://drive.google.com/uc?export=download&id=1BzwyQprRgGIsi60hTlIIF2r8O6pCK00A')

selected_breed_list = list(df.groupby('breed').count().sort_values(by='id', ascending=False).head(num_of_breeds).index)
filtered_df = df[df['breed'].isin(selected_breed_list)]
filtered_df['filename'] = df.apply(lambda label: ('data/train/' + label['id'] + '.jpg'), axis=1)

label_enc = LabelEncoder()
encoded_breeds = label_enc.fit_transform(filtered_df.breed.values)
print(filtered_df.head())


def load_image(img_path, size):
    # loads RGB image as PIL.Image.Image type
    img = load_img(img_path, target_size=size)
    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)
    x = img_to_array(img)
    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor
    return np.expand_dims(x, axis=0) / 255.


def load_images(df, size):
    list_of_tensors = [load_image(row.filename, size) for index, row in tqdm(df.iterrows())]
    return np.vstack(list_of_tensors)


def create_model():
    base_model = xception.Xception(weights='imagenet', include_top=False)
    # first: train only the top layers (which were randomly initialized)
    # i.e. freeze all convolutional Xception layers
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = BatchNormalization()(x)
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    x = Dense(1024, activation='relu')(x)
    x = Dropout(0.5)(x)

    predictions = Dense(num_of_breeds, activation='sigmoid')(x)

    # this is the model we will train
    model = Model(inputs=base_model.input, outputs=predictions)
    return model


model = create_model()

# Split into train, validation, test sets
X_train, X_test, y_train, y_test = train_test_split(filtered_df, encoded_breeds, test_size=0.1, random_state=5)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=5)

# load train, validation, test images
print("loading train images...")
train_images = load_images(X_train, (image_size, image_size))
print("finished loading train images...")
print("loading validation images...")
validation_images = load_images(X_val, (image_size, image_size))
print("finished loadingg validation images...")

train_datagen = ImageDataGenerator(rotation_range=45, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,
                                   zoom_range=0.25, horizontal_flip=True, fill_mode='nearest')
train_generator = train_datagen.flow(train_images, y_train, batch_size=batch_size, seed=seed)

validation_datagen = ImageDataGenerator()
validation_generator = validation_datagen.flow(validation_images, y_val, batch_size=batch_size, seed=seed)

model.compile(loss=sparse_categorical_crossentropy, optimizer='adam', metrics=['accuracy'])

train_step_size = train_generator.n // train_generator.batch_size
validation_step_size = validation_generator.n // validation_generator.batch_size

print("train step: ", train_step_size)
print("val step: ", validation_step_size)

hist = model.fit_generator(
    train_generator,
    steps_per_epoch=train_step_size,
    epochs=20,
    validation_data=validation_generator,
    validation_steps=validation_step_size)

scoreSeg = model.evaluate_generator(validation_generator, steps=validation_step_size)
print("Accuracy = %", scoreSeg[1] * 100)

# summarize history for accuracy
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# summarize history for loss
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

for i, row in X_test.iterrows():
    # load image
    img = load_image(row.filename, (image_size, image_size))
    # predict image
    predicted_class = np.argmax(model.predict(img))
    # get predicted index and decode labels
    breed_idx = encoded_breeds.tolist().index(predicted_class)
    decoded_breeds = label_enc.inverse_transform(encoded_breeds)
    # display predicted class
    predicted_label = decoded_breeds[breed_idx]
    print("predicted class: ", predicted_label)
    print("actual class: ", row.breed)
    # display image
    plt.imshow(img[0])
    plt.axis('off')
    plt.show()